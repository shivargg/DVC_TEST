{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation Notebook\n",
    "1. Load data from Snowflake.\n",
    "2. Performs relevant checks and operations (removing columns with one value, VIF checks, scaling) to get a dataset ready for modelling for the following agent feature - API.\n",
    "3. Stores all relevent data as a pkl object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read params from yaml file.\n",
    "params = yaml.safe_load(open('../params.yaml'))['prepare']\n",
    "\n",
    "MODEL_TYPE = params['MODEL_TYPE'] # 'API', 'APP_COUNT', 'FYC' or 'PERSISTENCY'\n",
    "TEST_SET_LENGTH = int(params['TEST_SET_LENGTH']) # In years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, MinMaxScaler, RobustScaler\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels .tools.tools import add_constant\n",
    "from pickle import dumps\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import copy\n",
    "import datetime, time\n",
    "import math\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 800)\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Zen of Python, by Tim Peters\n",
      "\n",
      "Beautiful is better than ugly.\n",
      "Explicit is better than implicit.\n",
      "Simple is better than complex.\n",
      "Complex is better than complicated.\n",
      "Flat is better than nested.\n",
      "Sparse is better than dense.\n",
      "Readability counts.\n",
      "Special cases aren't special enough to break the rules.\n",
      "Although practicality beats purity.\n",
      "Errors should never pass silently.\n",
      "Unless explicitly silenced.\n",
      "In the face of ambiguity, refuse the temptation to guess.\n",
      "There should be one-- and preferably only one --obvious way to do it.\n",
      "Although that way may not be obvious at first unless you're Dutch.\n",
      "Now is better than never.\n",
      "Although never is often better than *right* now.\n",
      "If the implementation is hard to explain, it's a bad idea.\n",
      "If the implementation is easy to explain, it may be a good idea.\n",
      "Namespaces are one honking great idea -- let's do more of those!\n"
     ]
    }
   ],
   "source": [
    "import snowflake_acc\n",
    "\n",
    "with open('uat_creds.json', 'r') as fp:\n",
    "    params = json.load(fp)\n",
    "\n",
    "acc = snowflake_acc.create_snowflake_accessor(params, globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_version_date_time_str = today.strftime(\"%m_%d_%y__%H_%M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_cols_with_one_unique_value(data):\n",
    "    cols_to_drop = []\n",
    "    all_cols = list(data.columns)\n",
    "    for col in all_cols:\n",
    "        if np.shape(data[col].unique())[0] == 1:\n",
    "            cols_to_drop.append(col)\n",
    "    data.drop(columns = cols_to_drop, inplace = True)\n",
    "    print(\"Columns dropped: \", cols_to_drop)\n",
    "    print(f\"Dropped {len(cols_to_drop)} columns from the dataframe.\")\n",
    "    return cols_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_list(target_value_to_predict):\n",
    "    return [f'{target_value_to_predict}_Q_MINUS_{i}' for i in range(1, 5)]\n",
    "\n",
    "\n",
    "def generate_train_test(data_orig, predictor_col):\n",
    "    data = copy.deepcopy(data_orig)\n",
    "    latest_quarter = pd.to_datetime(data['YEAR_QTR_DATE'].max())\n",
    "    # Remove the most current quarter data.\n",
    "    data = data[data['YEAR_QTR_DATE'] != latest_quarter]\n",
    "    # Construct the time point where we start splitting our data into train and test.\n",
    "    split_point = latest_quarter - pd.DateOffset(years= TEST_SET_LENGTH)\n",
    "    # Build X-Train and y-train.\n",
    "    train = data[data['YEAR_QTR_DATE'] < split_point]\n",
    "#     y_train = X_train[predictor_col]\n",
    "#     X_train.drop(columns = [predictor_col], inplace = True)\n",
    "    # Build X-Test and y-test.\n",
    "    test = data[(data['YEAR_QTR_DATE'] >= split_point)]\n",
    "#     y_test = X_test[predictor_col]\n",
    "#     X_test.drop(columns = [predictor_col], inplace = True)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def split_df_train(data, predictor_col):\n",
    "    qtr_to_split = pd.to_datetime(data['YEAR_QTR_DATE'].max()) - pd.DateOffset(months=3)\n",
    "    # Build X-Train and y-train.\n",
    "    X_train = data[data['YEAR_QTR_DATE'] < qtr_to_split]\n",
    "    y_train = X_train[predictor_col]\n",
    "    X_train.drop(columns = [predictor_col], inplace = True)\n",
    "    # Build X-Test and y-test.\n",
    "    X_test = data[data['YEAR_QTR_DATE'] >= qtr_to_split]\n",
    "    y_test = X_test[predictor_col]\n",
    "    X_test.drop(columns = [predictor_col], inplace = True)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def min_max_scale_X(data_train, data_test, cols_to_scale, test_set_passed):\n",
    "    scaler_train = MinMaxScaler().fit(data_train[cols_to_scale].values)\n",
    "    data_train[cols_to_scale] = pd.DataFrame(scaler_train.transform(data_train[cols_to_scale].values), columns=cols_to_scale, index=data_train.index).fillna(0)\n",
    "    if test_set_passed:\n",
    "        data_test[cols_to_scale] = pd.DataFrame(scaler_train.transform(data_test[cols_to_scale].values), columns=cols_to_scale, index=data_test.index).fillna(0)\n",
    "    return scaler_train\n",
    "\n",
    "\n",
    "def min_max_scale_y(data_train, data_test, test_set_passed):\n",
    "    scaler_train = MinMaxScaler().fit(data_train.values.reshape(-1, 1))\n",
    "    data_train = pd.DataFrame(scaler_train.transform(data_train.values.reshape(-1, 1)), index=data_train.index).fillna(0)\n",
    "    if test_set_passed:\n",
    "        data_test = pd.DataFrame(scaler_train.transform(data_test.values.reshape(-1, 1)), index=data_test.index).fillna(0)\n",
    "    return scaler_train\n",
    "    \n",
    "\n",
    "def scale_data(X_train, X_test, y_train, y_test, numeric_cols):\n",
    "    test_set_passed = X_test is not None and y_test is not None\n",
    "    if test_set_passed:\n",
    "        X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = X_train.copy(deep = True), X_test.copy(deep = True), y_train.copy(deep = True), y_test.copy(deep = True)\n",
    "        scaler_X_train = min_max_scale_X(X_train_scaled, X_test_scaled, numeric_cols, test_set_passed)\n",
    "#         scaler_y_train = min_max_scale_y(y_train_scaled, y_test_scaled, test_set_passed)\n",
    "        return scaler_X_train, None, X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled\n",
    "    else:\n",
    "        X_scaled, y_scaled = X_train.copy(deep = True), y_train.copy(deep = True)\n",
    "        scaler_X_train = min_max_scale_X(X_scaled, None, numeric_cols, test_set_passed)\n",
    "#         scaler_y_train = min_max_scale_y(y_scaled, None, test_set_passed)\n",
    "        return scaler_X_train, None, X_scaled, y_scaled\n",
    "\n",
    "\n",
    "def calc_max_vif(data):\n",
    "    vif = pd.DataFrame()\n",
    "    all_cols = data.columns\n",
    "    vif_values = [variance_inflation_factor(data.values, i) for i in range(len(all_cols))]\n",
    "    vif = pd.DataFrame({'column': all_cols, 'vif': vif_values})\n",
    "    max_vif_col = vif.loc[vif['vif'].idxmax()]\n",
    "    return max_vif_col\n",
    "\n",
    "\n",
    "def find_cols_to_remove_vif(X_train, numeric_cols):\n",
    "    vif_cols_dropped = []\n",
    "    cols_not_to_drop = ['Quarter__1', 'Quarter__2', 'Quarter__3', 'Quarter__4', 'AGT_F_CLI_COUNT', 'AGT_M_CLI_COUNT', 'NUM_ACTIVE_POLICIES']\n",
    "    X_train_VIF = X_train.drop(columns = cols_not_to_drop)\n",
    "    numeric_cols_clean = copy.deepcopy(numeric_cols)\n",
    "    for el in cols_not_to_drop:\n",
    "        numeric_cols_clean.remove(el)\n",
    "    while True:\n",
    "        highest_vif = calc_max_vif(X_train_VIF[numeric_cols_clean])\n",
    "        if highest_vif['vif'] > 5.0:\n",
    "            print(highest_vif)\n",
    "            col_to_remove = highest_vif['column']\n",
    "            X_train_VIF.drop(columns = [col_to_remove], inplace = True)\n",
    "            numeric_cols_clean.remove(col_to_remove)\n",
    "            vif_cols_dropped.append(col_to_remove)\n",
    "            print(\"\\n\")\n",
    "        else:\n",
    "            break\n",
    "    return vif_cols_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "\n",
    "def calculate_vif(X, thresh=100, verbose=False):\n",
    "    cols_to_drop = []\n",
    "    cols = X.columns\n",
    "    variables = np.arange(X.shape[1])\n",
    "    dropped = True\n",
    "    while dropped:\n",
    "        dropped = False\n",
    "        c = X[cols[variables]].values\n",
    "        vif = [variance_inflation_factor(c, ix) for ix in np.arange(c.shape[1])]\n",
    "        maxloc = vif.index(max(vif))\n",
    "        if max(vif) > thresh:\n",
    "            cols_to_drop.append(X[cols[variables]].columns[maxloc])\n",
    "            if verbose:\n",
    "                print('dropping \\'' + X[cols[variables]].columns[maxloc] + '\\' at index: ' + str(maxloc))\n",
    "            variables = np.delete(variables, maxloc)\n",
    "            dropped = True\n",
    "    if verbose:\n",
    "        print('Remaining variables:')\n",
    "        print(X.columns[variables])\n",
    "    return cols_to_drop #X[cols[variables]]\n",
    "\n",
    "\n",
    "def variance_threshold_selector(data, threshold=0.5):\n",
    "    # https://stackoverflow.com/a/39813304/1956309\n",
    "    selector = VarianceThreshold(threshold)\n",
    "    selector.fit(data)\n",
    "    return data[data.columns[selector.get_support(indices=True)]]\n",
    "\n",
    "\n",
    "def remove_correlated_features(df: pd.DataFrame, inplace=False):\n",
    "    corr_matrix = df.corr().abs()\n",
    "\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "    # Find features with correlation greater than 0.95\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "    if inplace:\n",
    "        df.drop(to_drop, axis=1, inplace=True)\n",
    "        return df, to_drop\n",
    "\n",
    "    df1 = df.drop(to_drop, axis=1, inplace=False)\n",
    "    return df1, to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quarter_to_predict(df, quarters_to_lookback):\n",
    "    all_quarters = df['YEAR_QTR_DATE'].unique()\n",
    "    all_quarters.sort()\n",
    "    quarter_to_predict = all_quarters[-1 * quarters_to_lookback]\n",
    "    return quarter_to_predict\n",
    "\n",
    "\n",
    "def extract_year_qtr(df, quarter_to_predict):\n",
    "    return df.query(f\"YEAR_QTR_DATE < '{pd.to_datetime(quarter_to_predict)}'\")\n",
    "\n",
    "\n",
    "def build_dataset_final(X, y, agt_ids, quarters_to_lookback):\n",
    "    train = copy.deepcopy(X)\n",
    "    train['y'] = y\n",
    "    train['AGT_ID'] = agt_ids\n",
    "    train.reset_index(inplace = True, drop = True)\n",
    "    quarter_to_predict = get_quarter_to_predict(X, 1)\n",
    "    print(\"QUARTER TO PREDICT -> \", quarter_to_predict)\n",
    "    train_filtered = extract_year_qtr(train, quarter_to_predict)\n",
    "    year_qtr_max = train_filtered['YEAR_QTR_DATE'].max()\n",
    "    X_train = train_filtered.drop(columns = ['YEAR_QTR_DATE', 'y', 'AGT_ID'])\n",
    "    y_train = train_filtered['y']\n",
    "    final = train.query(f\"YEAR_QTR_DATE >= '{quarter_to_predict}'\")\n",
    "    X_final = final.drop(columns = ['YEAR_QTR_DATE', 'y'])\n",
    "    print(X_final.shape)\n",
    "    return X_train, y_train, X_final, quarter_to_predict\n",
    "\n",
    "\n",
    "def get_current_year_quarter():\n",
    "    current_time = datetime.datetime.now()\n",
    "    month = current_time.month\n",
    "    return f'{current_time.year}_{math.floor(month/3) + 1}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track Columns Dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols_dropped = []\n",
    "all_cat_cols = []\n",
    "all_numeric_cols = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc.switch_schema('05_MODEL_INPUT_TOP_AGENT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing\n",
      "Processing table with  106 columns\n",
      "Processed  6860  rows\n"
     ]
    }
   ],
   "source": [
    "df_lagged = acc.retrieve_query_res(f'SELECT * FROM DS_GLOC_DEV_DB.\"05_MODEL_INPUT_TOP_AGENT\".\"05_AGENT_MASTER_TABLE_CLEANED\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing\n",
      "Processing table with  106 columns\n",
      "Processed  7486  rows\n"
     ]
    }
   ],
   "source": [
    "df_score = acc.retrieve_query_res(f'SELECT * FROM DS_GLOC_DEV_DB.\"05_MODEL_INPUT_TOP_AGENT\".\"05_AGENT_MASTER_TABLE_CLEANED_SCORING\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.fromtimestamp(time.time())\n",
    "curr_quarter = (now.month-1)//3+1\n",
    "curr_year = now.year\n",
    "\n",
    "data_quarter = int(df_lagged['YEAR_QUARTER'].max().split('_')[-1])\n",
    "data_quarter_date = df_lagged['YEAR_QUARTER'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019_1'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lagged['YEAR_QUARTER'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022_3'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_quarter_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(581, 106)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lagged[df_lagged['YEAR_QUARTER'] == data_quarter_date].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Removing last year quarter data from df_lagged.\")\n",
    "# df_lagged = df_lagged[df_lagged['YEAR_QUARTER'] != data_quarter_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding the last year quarter from df_scoring.\n"
     ]
    }
   ],
   "source": [
    "# Add the last quarter from the scoring set.\n",
    "print(\"Adding the last year quarter from df_scoring.\")\n",
    "df_score = df_score[df_score['YEAR_QUARTER'] == data_quarter_date]\n",
    "df_score['YEAR_QUARTER'] = f'{curr_year}_{curr_quarter}'\n",
    "df_score['QUARTER'] = curr_quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(598, 106)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022_3'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lagged['YEAR_QUARTER'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019_1'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lagged['YEAR_QUARTER'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022_4'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_score['YEAR_QUARTER'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022_4'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_score['YEAR_QUARTER'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(598, 106)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6860, 106)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lagged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_lagged, df_score], ignore_index = True, join = 'inner', verify_integrity = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7458, 106)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7458 entries, 0 to 7457\n",
      "Data columns (total 106 columns):\n",
      " #    Column                            Non-Null Count  Dtype  \n",
      "---   ------                            --------------  -----  \n",
      " 0    AGT_ID                            7458 non-null   object \n",
      " 1    AGT_GIV_NM                        7155 non-null   object \n",
      " 2    AGT_SUR_NM                        7458 non-null   object \n",
      " 3    AGT_FULL_NAME                     7155 non-null   object \n",
      " 4    AGT_FULL_NAME_NON_NULL            7458 non-null   object \n",
      " 5    AGT_BRANCH                        7458 non-null   object \n",
      " 6    AGT_AGE                           7458 non-null   object \n",
      " 7    AGT_INCM_EARN_AMT                 7458 non-null   int64  \n",
      " 8    AGT_EMPL_YR_QTY                   7458 non-null   object \n",
      " 9    AGT_SEX_F                         7458 non-null   int64  \n",
      " 10   AGT_SEX_M                         7458 non-null   int64  \n",
      " 11   AGT_SEX_C                         7458 non-null   int64  \n",
      " 12   CLI_OCCP_CLAS_1                   7458 non-null   int64  \n",
      " 13   CLI_OCCP_CLAS_2                   7458 non-null   int64  \n",
      " 14   CLI_OCCP_CLAS_MISSING             7458 non-null   int64  \n",
      " 15   AGT_MARIT_STAT_S                  7458 non-null   int64  \n",
      " 16   AGT_MARIT_STAT_C                  7458 non-null   int64  \n",
      " 17   AGT_MARIT_STAT_W                  7458 non-null   int64  \n",
      " 18   AGT_MARIT_STAT_M                  7458 non-null   int64  \n",
      " 19   AGT_MARIT_STAT_D                  7458 non-null   int64  \n",
      " 20   AGT_MARIT_STAT_P                  7458 non-null   int64  \n",
      " 21   AGT_MARIT_STAT_MISSING            7458 non-null   int64  \n",
      " 22   AGT_REGION_SIPARIA                7458 non-null   int64  \n",
      " 23   AGT_REGION_SANGRE_GRANDE          7458 non-null   int64  \n",
      " 24   AGT_REGION_TOBAGO                 7458 non-null   int64  \n",
      " 25   AGT_REGION_PENALDEBE              7458 non-null   int64  \n",
      " 26   AGT_REGION_SAN_JUAN_LAVENTILLE    7458 non-null   int64  \n",
      " 27   AGT_REGION_TUNAPUNAPIARCO         7458 non-null   int64  \n",
      " 28   AGT_REGION_CHAGUANAS              7458 non-null   int64  \n",
      " 29   AGT_REGION_COUVATABAQUITETALPARO  7458 non-null   int64  \n",
      " 30   AGT_REGION_PRINCES_TOWN           7458 non-null   int64  \n",
      " 31   AGT_REGION_POINT_FORTIN           7458 non-null   int64  \n",
      " 32   AGT_REGION_PORT_OF_SPAIN          7458 non-null   int64  \n",
      " 33   AGT_REGION_MISSING                7458 non-null   int64  \n",
      " 34   AGT_REGION_DIEGO_MARTIN           7458 non-null   int64  \n",
      " 35   AGT_REGION_ARIMA                  7458 non-null   int64  \n",
      " 36   AGT_REGION_SAN_FERNANDO           7458 non-null   int64  \n",
      " 37   AGT_REGION_MAYARORIO_CLARO        7458 non-null   int64  \n",
      " 38   AGT_REGION_OTHER                  7458 non-null   int64  \n",
      " 39   AGT_PART_TIME_FLAG                7458 non-null   int64  \n",
      " 40   AGT_AGE_TEMPORAL                  7458 non-null   object \n",
      " 41   YEAR                              7458 non-null   int64  \n",
      " 42   QUARTER                           7458 non-null   int64  \n",
      " 43   YEAR_QUARTER                      7458 non-null   object \n",
      " 44   APP_COUNT                         7458 non-null   object \n",
      " 45   FYC                               7458 non-null   object \n",
      " 46   PERSISTENCY                       7458 non-null   object \n",
      " 47   API                               7458 non-null   object \n",
      " 48   APP_COUNT_Q_MINUS_1               7458 non-null   object \n",
      " 49   APP_COUNT_Q_MINUS_2               7458 non-null   object \n",
      " 50   APP_COUNT_Q_MINUS_3               7458 non-null   object \n",
      " 51   APP_COUNT_Q_MINUS_4               7458 non-null   object \n",
      " 52   FYC_Q_MINUS_1                     7458 non-null   object \n",
      " 53   FYC_Q_MINUS_2                     7458 non-null   object \n",
      " 54   FYC_Q_MINUS_3                     7458 non-null   object \n",
      " 55   FYC_Q_MINUS_4                     7458 non-null   object \n",
      " 56   PERSISTENCY_Q_MINUS_1             7458 non-null   object \n",
      " 57   PERSISTENCY_Q_MINUS_2             7458 non-null   object \n",
      " 58   PERSISTENCY_Q_MINUS_3             7458 non-null   object \n",
      " 59   PERSISTENCY_Q_MINUS_4             7458 non-null   object \n",
      " 60   API_Q_MINUS_1                     7458 non-null   object \n",
      " 61   API_Q_MINUS_2                     7458 non-null   object \n",
      " 62   API_Q_MINUS_3                     7458 non-null   object \n",
      " 63   API_Q_MINUS_4                     7458 non-null   object \n",
      " 64   AGENT_TENURE                      7458 non-null   int64  \n",
      " 65   COUNT_3                           7456 non-null   float64\n",
      " 66   COUNT_7                           7456 non-null   float64\n",
      " 67   COUNT_8                           7456 non-null   float64\n",
      " 68   COUNT_9                           7456 non-null   float64\n",
      " 69   COUNT_C                           7456 non-null   float64\n",
      " 70   COUNT_D                           7456 non-null   float64\n",
      " 71   COUNT_F                           7456 non-null   float64\n",
      " 72   COUNT_M                           7456 non-null   float64\n",
      " 73   COUNT_N                           7456 non-null   float64\n",
      " 74   COUNT_V                           7456 non-null   float64\n",
      " 75   AGT_CLI_MEDIAN_YRS_EMPLOYED       7458 non-null   object \n",
      " 76   AGT_CLI_AVG_YRS_EMPLOYED          7458 non-null   float64\n",
      " 77   AGT_CLI_MEDIAN_INCOME             7458 non-null   object \n",
      " 78   AGT_CLI_AVG_INCOME                7458 non-null   object \n",
      " 79   AGT_F_CLI_COUNT                   7458 non-null   int64  \n",
      " 80   AGT_M_CLI_COUNT                   7458 non-null   int64  \n",
      " 81   AGT_O_CLI_COUNT                   7458 non-null   int64  \n",
      " 82   NUM_SURRENDERED_POLICIES          7458 non-null   int64  \n",
      " 83   NUM_REJECTED_POLICIES             7458 non-null   int64  \n",
      " 84   NUM_REDATED_POLICIES              7458 non-null   int64  \n",
      " 85   NUM_LAPSED_POLICIES               7458 non-null   int64  \n",
      " 86   NUM_ACTIVE_POLICIES               7458 non-null   int64  \n",
      " 87   NUM_PENDING_POLICIES              7458 non-null   int64  \n",
      " 88   CVG_FACE_AMT                      7458 non-null   object \n",
      " 89   CVG_ORIG_FACE_AMT                 7458 non-null   object \n",
      " 90   CVG_PREV_FACE_AMT                 7458 non-null   object \n",
      " 91   CVG_UWG_AMT                       7458 non-null   object \n",
      " 92   CVG_UNIT_VALU_AMT                 7458 non-null   int64  \n",
      " 93   CVG_SUM_INS_AMT                   7458 non-null   object \n",
      " 94   CVG_MPREM_AMT                     7458 non-null   object \n",
      " 95   CVG_PFEE_AMT                      7458 non-null   object \n",
      " 96   CVG_BASIC_PREM_AMT                7458 non-null   object \n",
      " 97   CVG_MDRT_AMT                      7458 non-null   int64  \n",
      " 98   CVG_FYR_COMM_AMT                  7458 non-null   object \n",
      " 99   IN_ALLOC_AMT_PCT                  7458 non-null   object \n",
      " 100  CVG_PMT_LTD_AMT                   7458 non-null   object \n",
      " 101  MNPMT_TRG_LTD_AMT                 7458 non-null   int64  \n",
      " 102  SURR_LOAD_LTD_AMT                 7458 non-null   object \n",
      " 103  CVG_SURR_LTD_AMT                  7458 non-null   object \n",
      " 104  CVG_GDLN_APREM_AMT                7458 non-null   object \n",
      " 105  DATE_OF_BIRTH                     3427 non-null   object \n",
      "dtypes: float64(11), int64(47), object(48)\n",
      "memory usage: 6.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info(verbose = True, show_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['AGT_SEX_F', 'AGT_SEX_M',\n",
    "            'CLI_OCCP_CLAS_1', 'CLI_OCCP_CLAS_2',\n",
    "            'AGT_MARIT_STAT_S', 'AGT_MARIT_STAT_C', 'AGT_MARIT_STAT_W', 'AGT_MARIT_STAT_M', 'AGT_MARIT_STAT_D', 'AGT_MARIT_STAT_P',\n",
    "            'AGT_REGION_TUNAPUNAPIARCO', 'AGT_REGION_TOBAGO', 'AGT_REGION_SIPARIA', 'AGT_REGION_SAN_JUAN_LAVENTILLE', 'AGT_REGION_SAN_FERNANDO',\n",
    "            'AGT_REGION_SANGRE_GRANDE', 'AGT_REGION_PRINCES_TOWN', 'AGT_REGION_PORT_OF_SPAIN', 'AGT_REGION_POINT_FORTIN', 'AGT_REGION_PENALDEBE',\n",
    "            'AGT_REGION_OTHER', 'AGT_REGION_MISSING', 'AGT_REGION_MAYARORIO_CLARO', 'AGT_REGION_DIEGO_MARTIN', 'AGT_REGION_COUVATABAQUITETALPARO',\n",
    "            'AGT_REGION_CHAGUANAS', 'AGT_REGION_ARIMA',\n",
    "            'AGT_PART_TIME_FLAG'\n",
    "]\n",
    "\n",
    "cols_to_drop = ['AGT_SEX_C', \n",
    "                'CLI_OCCP_CLAS_MISSING', \n",
    "                'AGT_MARIT_STAT_MISSING', \n",
    "                'DATE_OF_BIRTH',\n",
    "                'AGT_FULL_NAME', 'AGT_FULL_NAME_NON_NULL', 'AGT_GIV_NM', 'AGT_SUR_NM', \n",
    "                'YEAR', 'MONTH', 'YEAR_QUARTER', \n",
    "                'AGT_AGE', 'AGT_BRANCH'\n",
    "]\n",
    "\n",
    "cols_to_drop_fluid = [\n",
    "                'CNTRCT_TRMN_DT_TXT', 'AGT_STAT_CD', 'START_MONTH', 'START_YEAR',\n",
    "                'FEEDBACK', 'MAX(CFCVG.POL_ID)', 'MAX(CFCVG.CVG_NUM)',\n",
    "#                 'AGT_M_CLI_COUNT', 'AGT_F_CLI_COUNT', 'AGT_O_CLI_COUNT', \n",
    "                'AGT_TOTAL_CLI_INCM_EARNED', 'AGT_CLI_AVG_EMPL_AGE', 'AGT_CLI_OCCP_CLAS_CD_COUNT', 'AGT_CLI_MARIT_STAT_CD_COUNT',\n",
    "                'CVG_FACE_AMT', 'CVG_ORIG_FACE_AMT', 'CVG_PREV_FACE_AMT', 'CVG_UWG_AMT', 'CVG_UNIT_VALU_AMT', 'CVG_SUM_INS_AMT', 'CVG_AD_FACE_AMT', 'CVG_MPREM_AMT', 'CVG_PFEE_AMT',\n",
    "                'CVG_BASIC_PREM_AMT', 'CVG_AD_PREM_AMT', 'CVG_WP_PREM_AMT', 'REDC_EP_PREM_AMT', 'OWN_OCCP_PREM_AMT', 'CVG_LTA_PREM_AMT', 'CVG_LTB_PREM_AMT', 'PDISAB_PREM_AMT', 'CVG_COLA_PREM_AMT',\n",
    "                'CVG_FE_UPREM_AMT', 'CVG_FE_PREM_AMT', 'CVG_ME_PREM_AMT', 'CVG_SALE_TAX_AMT', 'PREV_WP_UPREM_AMT', 'CVG_PREV_UPREM_AMT', 'CVG_NXT_UPREM_AMT', 'CVG_MDRT_AMT', 'CVG_FYR_COMM_AMT',\n",
    "                'CVG_CLM_YTD_AMT', 'CVG_CLM_LTD_AMT', 'CVG_CLM_CHQ_AMT', 'CVG_WP_YTD_AMT', 'CVG_WP_LTD_AMT', 'CVG_NET_REISS_AMT', \n",
    "                'IN_ALLOC_AMT_PCT', 'OUT_ALLOC_AMT_PCT', 'CVG_MAX_COMIT_AMT',\n",
    "                'CVG_COMM_TRG_AMT', 'PMT_LOAD_TRG_AMT', 'PMT_LOAD_LTD_AMT', 'CVG_PMT_LTD_AMT', 'MNPMT_TRG_LTD_AMT', 'CVG_SURR_TRG_AMT', 'SURR_LOAD_LTD_AMT', 'CVG_SURR_LTD_AMT',\n",
    "                'CVG_GDLN_APREM_AMT', 'CVG_GDLN_SPREM_AMT', 'CVG_LOAN_CLR_1_AMT', 'CVG_LOAN_CLR_2_AMT', 'CVG_APL_CLR_AMT', 'GIR_OPT_REMN_AMT', 'CVG_FE2_UPREM_AMT', '2018_TOTAL_BONUS_RATE',\n",
    "                '2018_TOTAL_ADD_BONUS_RATE', '2019_TOTAL_BONUS_RATE', '2019_TOTAL_ADD_BONUS_RATE', '2020_TOTAL_BONUS_RATE', '2020_TOTAL_ADD_BONUS_RATE', '2021_TOTAL_BONUS_RATE', '2021_TOTAL_ADD_BONUS_RATE',\n",
    "                '2018_FYR_COMM_CMO_AMT', '2018_FYR_COMM_YTD_AMT', '2018_RENW_COMM_CMO_AMT', '2018_RENW_COMM_YTD_AMT', '2018_OVRID_COMM_CMO_AMT', '2018_OVRID_COMM_YTD_AMT', '2018_AGT_PAYO_CMO_AMT',\n",
    "                '2018_AGT_PAYO_YTD_AMT', '2018_FYR_LCOMM_CMO_AMT', '2018_FYR_LCOMM_YTD_AMT', '2018_RENW_LCOMM_CMO_AMT', '2018_RENW_LCOMM_YTD_AMT', '2018_QLTY_BON_CMO_AMT', '2018_QLTY_BON_YTD_AMT',\n",
    "                '2018_MKT_BON_CMO_AMT', '2018_MKT_BON_YTD_AMT', '2018_NHS_ALLOW_CMO_AMT', '2018_NHS_ALLOW_YTD_AMT', '2018_AGT_APP_PMO_QTY', '2018_AGT_APP_YTM_QTY', '2018_AGT_PWRIT_PMO_AMT',\n",
    "                '2018_AGT_PWRIT_YTM_AMT', '2018_LIFE_PWRIT_PMO_AMT', '2018_LIFE_PWRIT_YTM_AMT', '2018_FYR_CPREM_CMO_AMT', '2018_FYR_CPREM_YTD_AMT', '2018_RENW_CPREM_CMO_AMT',\n",
    "                '2018_RENW_CPREM_YTD_AMT', '2018_TOTAL_FYR_COMM', '2018_TOTAL_RENW_COMM', '2019_FYR_COMM_CMO_AMT', '2019_FYR_COMM_YTD_AMT', '2019_RENW_COMM_CMO_AMT', '2019_RENW_COMM_YTD_AMT',\n",
    "                '2019_OVRID_COMM_CMO_AMT', '2019_OVRID_COMM_YTD_AMT', '2019_AGT_PAYO_CMO_AMT', '2019_AGT_PAYO_YTD_AMT', '2019_FYR_LCOMM_CMO_AMT', '2019_FYR_LCOMM_YTD_AMT', '2019_RENW_LCOMM_CMO_AMT',\n",
    "                '2019_RENW_LCOMM_YTD_AMT', '2019_QLTY_BON_CMO_AMT', '2019_QLTY_BON_YTD_AMT', '2019_MKT_BON_CMO_AMT', '2019_MKT_BON_YTD_AMT', '2019_NHS_ALLOW_CMO_AMT', '2019_NHS_ALLOW_YTD_AMT',\n",
    "                '2019_AGT_APP_PMO_QTY', '2019_AGT_APP_YTM_QTY', '2019_AGT_PWRIT_PMO_AMT', '2019_AGT_PWRIT_YTM_AMT', '2019_LIFE_PWRIT_PMO_AMT', '2019_LIFE_PWRIT_YTM_AMT', '2019_FYR_CPREM_CMO_AMT',\n",
    "                '2019_FYR_CPREM_YTD_AMT', '2019_RENW_CPREM_CMO_AMT', '2019_RENW_CPREM_YTD_AMT', '2019_TOTAL_FYR_COMM', '2019_TOTAL_RENW_COMM', '2020_FYR_COMM_CMO_AMT', '2020_FYR_COMM_YTD_AMT', '2020_RENW_COMM_CMO_AMT',\n",
    "                '2020_RENW_COMM_YTD_AMT', '2020_OVRID_COMM_CMO_AMT', '2020_OVRID_COMM_YTD_AMT', '2020_AGT_PAYO_CMO_AMT', '2020_AGT_PAYO_YTD_AMT', '2020_FYR_LCOMM_CMO_AMT', '2020_FYR_LCOMM_YTD_AMT',\n",
    "                '2020_RENW_LCOMM_CMO_AMT', '2020_RENW_LCOMM_YTD_AMT', '2020_QLTY_BON_CMO_AMT', '2020_QLTY_BON_YTD_AMT', '2020_MKT_BON_CMO_AMT', '2020_MKT_BON_YTD_AMT', '2020_NHS_ALLOW_CMO_AMT',\n",
    "                '2020_NHS_ALLOW_YTD_AMT', '2020_AGT_APP_PMO_QTY', '2020_AGT_APP_YTM_QTY', '2020_AGT_PWRIT_PMO_AMT', '2020_AGT_PWRIT_YTM_AMT', '2020_LIFE_PWRIT_PMO_AMT', '2020_LIFE_PWRIT_YTM_AMT',\n",
    "                '2020_FYR_CPREM_CMO_AMT', '2020_FYR_CPREM_YTD_AMT', '2020_RENW_CPREM_CMO_AMT', '2020_RENW_CPREM_YTD_AMT', '2020_TOTAL_FYR_COMM', '2020_TOTAL_RENW_COMM', '2021_FYR_COMM_CMO_AMT',\n",
    "                '2021_FYR_COMM_YTD_AMT', '2021_RENW_COMM_CMO_AMT', '2021_RENW_COMM_YTD_AMT', '2021_OVRID_COMM_CMO_AMT', '2021_OVRID_COMM_YTD_AMT', '2021_AGT_PAYO_CMO_AMT', '2021_AGT_PAYO_YTD_AMT',\n",
    "                '2021_FYR_LCOMM_CMO_AMT', '2021_FYR_LCOMM_YTD_AMT', '2021_RENW_LCOMM_CMO_AMT', '2021_RENW_LCOMM_YTD_AMT', '2021_QLTY_BON_CMO_AMT', '2021_QLTY_BON_YTD_AMT', '2021_MKT_BON_CMO_AMT', '2021_MKT_BON_YTD_AMT',\n",
    "                '2021_NHS_ALLOW_CMO_AMT', '2021_NHS_ALLOW_YTD_AMT', '2021_AGT_APP_PMO_QTY', '2021_AGT_APP_YTM_QTY', '2021_AGT_PWRIT_PMO_AMT', '2021_AGT_PWRIT_YTM_AMT', '2021_LIFE_PWRIT_PMO_AMT', '2021_LIFE_PWRIT_YTM_AMT',\n",
    "                '2021_FYR_CPREM_CMO_AMT', '2021_FYR_CPREM_YTD_AMT', '2021_RENW_CPREM_CMO_AMT', '2021_RENW_CPREM_YTD_AMT', '2021_TOTAL_FYR_COMM', '2021_TOTAL_RENW_COMM', '3','7', '8',\n",
    "                '9', 'C', 'D', 'F', 'M', 'N', 'V'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols_to_drop_clean = [el for el in cols_to_drop_fluid if 'CVG' not in el]\n",
    "cols_to_drop_clean = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations to Prepare Data:\n",
    "1. Cast columns to numeric form. ()\n",
    "2. Extract quarter. ()\n",
    "3. Dropping cols (multiple times). ()\n",
    "4. Drop rows with missing data. ()\n",
    "5. Build list of numeric columns. ()\n",
    "6. Drop predictor variables. ()\n",
    "7. Split dataset into train and prediction sets. ()\n",
    "8. Split train dataset into train and test segments. ()\n",
    "9. Apply scaling to split data. ()\n",
    "10. VIF removal. ()\n",
    "11. Drop agent feature from datasets. ()\n",
    "12. Save as pickle. ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For validation/testing - remove the current year_quarter value.\n",
    "# For production, include the current year_quarter_value.\n",
    "\n",
    "def prepare_data_walk_forward(df_original, selected_predictor):\n",
    "    # Copy original dataframe.\n",
    "    df = copy.deepcopy(df_original)\n",
    "    \n",
    "    # Attempt to cast all columns to numeric form.\n",
    "    df[df.columns] = df[df.columns].apply(pd.to_numeric, errors='ignore')\n",
    "    \n",
    "    # Applying transformations to extract the quarter.\n",
    "    df['YEAR'] = df['YEAR_QUARTER'].str.split('_').str.get(0)\n",
    "    df['MONTH'] = df['YEAR_QUARTER'].str.split('_').str.get(1).map({'1': '1', '2': '4', '3': '7', '4': '10'})\n",
    "    df['YEAR_QTR_DATE'] = pd.to_datetime(df['YEAR'] + '/' + df['MONTH'] + '/01', format = '%Y/%m/%d')\n",
    "    df['QUARTER'] = df['YEAR_QTR_DATE'].dt.quarter\n",
    "    quarter_dummies = pd.get_dummies(df['QUARTER'], prefix='Quarter_')\n",
    "    df = pd.merge(\n",
    "        left = df,\n",
    "        right = quarter_dummies,\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "    df.drop(columns = ['QUARTER'], inplace = True)\n",
    "    \n",
    "    # Drop initial set of columns.\n",
    "    df.drop(columns = cols_to_drop, inplace = True)\n",
    "    all_cols_dropped.append(cols_to_drop)\n",
    "    \n",
    "    # Drop columns with one value.\n",
    "    cols_dropped = remove_cols_with_one_unique_value(df)\n",
    "    all_cols_dropped.append(cols_dropped)\n",
    "    \n",
    "    # Drop additional columns.\n",
    "    additional_cols_to_consider_removing = list(set(list(df.columns)).intersection(set(cols_to_drop_clean)))\n",
    "    df.drop(columns = additional_cols_to_consider_removing, inplace = True)\n",
    "    all_cols_dropped.append(additional_cols_to_consider_removing)\n",
    "    \n",
    "    # Drop any rows with missing data.\n",
    "    df.dropna(axis=0, how='any', inplace = True)\n",
    "    \n",
    "    # Build the list of numeric columns.\n",
    "    numeric_cols = set(df.columns).difference(set(cat_cols))\n",
    "    numeric_cols.remove('AGT_ID')\n",
    "    numeric_cols.remove('YEAR_QTR_DATE')\n",
    "    numeric_cols = list(numeric_cols)\n",
    "    \n",
    "    # Drop predictor variables from dataset except the selected_predictor.\n",
    "    all_predictors = ['API', 'PERSISTENCY', 'APP_COUNT', 'FYC']\n",
    "    for predictor in all_predictors:\n",
    "        numeric_cols.remove(predictor)\n",
    "    all_predictors.remove(MODEL_TYPE)\n",
    "    df.drop(columns = all_predictors, inplace = True)\n",
    "    all_cols_dropped.append(all_predictors)\n",
    "    \n",
    "    # We want to create the initial train and test datasets.\n",
    "    train, test = generate_train_test(df, selected_predictor)\n",
    "    \n",
    "    # Run VIF for training data.\n",
    "    # vif_cols_5 = calculate_vif(train.drop(columns = ['AGT_ID', 'YEAR_QTR_DATE', selected_predictor]), 5)\n",
    "    vif_cols_5 = None\n",
    "    \n",
    "    dataset = {\n",
    "        'train': train,\n",
    "        'test': test,\n",
    "        'vif_cols_5': vif_cols_5,\n",
    "        'numeric_cols': numeric_cols,\n",
    "        'cat_cols': cat_cols,\n",
    "        'dropped_cols': all_cols_dropped,\n",
    "        'prod': df\n",
    "    }\n",
    "    \n",
    "    print(\"Ranges for train data: \", train['YEAR_QTR_DATE'].min(), train['YEAR_QTR_DATE'].max())\n",
    "    print(\"Ranges for test data: \", train['YEAR_QTR_DATE'].min(), train['YEAR_QTR_DATE'].max())\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Pickle Files / Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataset for API.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19612/3166703314.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['MONTH'] = df['YEAR_QUARTER'].str.split('_').str.get(1).map({'1': '1', '2': '4', '3': '7', '4': '10'})\n",
      "/tmp/ipykernel_19612/3166703314.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['YEAR_QTR_DATE'] = pd.to_datetime(df['YEAR'] + '/' + df['MONTH'] + '/01', format = '%Y/%m/%d')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns dropped:  ['CLI_OCCP_CLAS_2', 'AGT_MARIT_STAT_C', 'AGT_MARIT_STAT_W', 'AGT_REGION_SANGRE_GRANDE', 'AGT_REGION_PENALDEBE', 'AGT_REGION_SAN_JUAN_LAVENTILLE', 'AGT_REGION_TUNAPUNAPIARCO', 'AGT_REGION_COUVATABAQUITETALPARO', 'AGT_REGION_PRINCES_TOWN', 'AGT_REGION_POINT_FORTIN', 'AGT_REGION_PORT_OF_SPAIN', 'AGT_REGION_DIEGO_MARTIN', 'AGT_REGION_SAN_FERNANDO', 'AGT_REGION_MAYARORIO_CLARO']\n",
      "Dropped 14 columns from the dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shiv/Github/DVC_TEST/env/lib/python3.8/site-packages/statsmodels/regression/linear_model.py:1752: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return 1 - self.ssr/self.centered_tss\n",
      "/home/shiv/Github/DVC_TEST/env/lib/python3.8/site-packages/statsmodels/regression/linear_model.py:1754: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return 1 - self.ssr/self.uncentered_tss\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuilding dataset for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_TYPE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m model_prep \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data_walk_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL_TYPE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_TYPE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_dataset.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[1;32m      6\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(model_prep, handle, protocol\u001b[38;5;241m=\u001b[39mpickle\u001b[38;5;241m.\u001b[39mHIGHEST_PROTOCOL)\n",
      "Cell \u001b[0;32mIn[38], line 59\u001b[0m, in \u001b[0;36mprepare_data_walk_forward\u001b[0;34m(df_original, selected_predictor)\u001b[0m\n\u001b[1;32m     56\u001b[0m train, test \u001b[38;5;241m=\u001b[39m generate_train_test(df, selected_predictor)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Run VIF for training data.\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m vif_cols_5 \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_vif\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAGT_ID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mYEAR_QTR_DATE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_predictor\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m dataset \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: train,\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m: test,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropped_cols\u001b[39m\u001b[38;5;124m'\u001b[39m: all_cols_dropped\n\u001b[1;32m     68\u001b[0m }\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRanges for train data: \u001b[39m\u001b[38;5;124m\"\u001b[39m, train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYEAR_QTR_DATE\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmin(), train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYEAR_QTR_DATE\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax())\n",
      "Cell \u001b[0;32mIn[13], line 17\u001b[0m, in \u001b[0;36mcalculate_vif\u001b[0;34m(X, thresh, verbose)\u001b[0m\n\u001b[1;32m     15\u001b[0m dropped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     16\u001b[0m c \u001b[38;5;241m=\u001b[39m X[cols[variables]]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m---> 17\u001b[0m vif \u001b[38;5;241m=\u001b[39m [variance_inflation_factor(c, ix) \u001b[38;5;28;01mfor\u001b[39;00m ix \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39marange(c\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])]\n\u001b[1;32m     18\u001b[0m maxloc \u001b[38;5;241m=\u001b[39m vif\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mmax\u001b[39m(vif))\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(vif) \u001b[38;5;241m>\u001b[39m thresh:\n",
      "Cell \u001b[0;32mIn[13], line 17\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     15\u001b[0m dropped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     16\u001b[0m c \u001b[38;5;241m=\u001b[39m X[cols[variables]]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m---> 17\u001b[0m vif \u001b[38;5;241m=\u001b[39m [\u001b[43mvariance_inflation_factor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mix\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m ix \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39marange(c\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])]\n\u001b[1;32m     18\u001b[0m maxloc \u001b[38;5;241m=\u001b[39m vif\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mmax\u001b[39m(vif))\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(vif) \u001b[38;5;241m>\u001b[39m thresh:\n",
      "File \u001b[0;32m~/Github/DVC_TEST/env/lib/python3.8/site-packages/statsmodels/stats/outliers_influence.py:194\u001b[0m, in \u001b[0;36mvariance_inflation_factor\u001b[0;34m(exog, exog_idx)\u001b[0m\n\u001b[1;32m    192\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(k_vars) \u001b[38;5;241m!=\u001b[39m exog_idx\n\u001b[1;32m    193\u001b[0m x_noti \u001b[38;5;241m=\u001b[39m exog[:, mask]\n\u001b[0;32m--> 194\u001b[0m r_squared_i \u001b[38;5;241m=\u001b[39m \u001b[43mOLS\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_noti\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfit()\u001b[38;5;241m.\u001b[39mrsquared\n\u001b[1;32m    195\u001b[0m vif \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m-\u001b[39m r_squared_i)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vif\n",
      "File \u001b[0;32m~/Github/DVC_TEST/env/lib/python3.8/site-packages/statsmodels/regression/linear_model.py:906\u001b[0m, in \u001b[0;36mOLS.__init__\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    903\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights are not supported in OLS and will be ignored\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    904\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn exception will be raised in the next version.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    905\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg, ValueWarning)\n\u001b[0;32m--> 906\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mOLS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_keys:\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_keys\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Github/DVC_TEST/env/lib/python3.8/site-packages/statsmodels/regression/linear_model.py:733\u001b[0m, in \u001b[0;36mWLS.__init__\u001b[0;34m(self, endog, exog, weights, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    732\u001b[0m     weights \u001b[38;5;241m=\u001b[39m weights\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m--> 733\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mWLS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    735\u001b[0m nobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    736\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights\n",
      "File \u001b[0;32m~/Github/DVC_TEST/env/lib/python3.8/site-packages/statsmodels/regression/linear_model.py:190\u001b[0m, in \u001b[0;36mRegressionModel.__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mRegressionModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_attr\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpinv_wexog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwendog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwexog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/Github/DVC_TEST/env/lib/python3.8/site-packages/statsmodels/base/model.py:267\u001b[0m, in \u001b[0;36mLikelihoodModel.__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize()\n",
      "File \u001b[0;32m~/Github/DVC_TEST/env/lib/python3.8/site-packages/statsmodels/base/model.py:92\u001b[0m, in \u001b[0;36mModel.__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m missing \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     91\u001b[0m hasconst \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhasconst\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_constant \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mk_constant\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mexog\n",
      "File \u001b[0;32m~/Github/DVC_TEST/env/lib/python3.8/site-packages/statsmodels/base/model.py:132\u001b[0m, in \u001b[0;36mModel._handle_data\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_handle_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, missing, hasconst, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 132\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mhandle_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# kwargs arrays could have changed, easier to just attach here\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "File \u001b[0;32m~/Github/DVC_TEST/env/lib/python3.8/site-packages/statsmodels/base/data.py:700\u001b[0m, in \u001b[0;36mhandle_data\u001b[0;34m(endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    697\u001b[0m     exog \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(exog)\n\u001b[1;32m    699\u001b[0m klass \u001b[38;5;241m=\u001b[39m handle_data_class_factory(endog, exog)\n\u001b[0;32m--> 700\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mklass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m             \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/DVC_TEST/env/lib/python3.8/site-packages/statsmodels/base/data.py:88\u001b[0m, in \u001b[0;36mModelData.__init__\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconst_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_constant \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_constant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity()\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/Github/DVC_TEST/env/lib/python3.8/site-packages/statsmodels/base/data.py:177\u001b[0m, in \u001b[0;36mModelData._handle_constant\u001b[0;34m(self, hasconst)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_implicit \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m hasconst:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# look for implicit constant\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# Compute rank of augmented matrix\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     augmented_exog \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcolumn_stack(\n\u001b[1;32m    176\u001b[0m                 (np\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog))\n\u001b[0;32m--> 177\u001b[0m     rank_augm \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatrix_rank\u001b[49m\u001b[43m(\u001b[49m\u001b[43maugmented_exog\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     rank_orig \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mmatrix_rank(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog)\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_constant \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(rank_orig \u001b[38;5;241m==\u001b[39m rank_augm)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mmatrix_rank\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/Github/DVC_TEST/env/lib/python3.8/site-packages/numpy/linalg/linalg.py:1883\u001b[0m, in \u001b[0;36mmatrix_rank\u001b[0;34m(A, tol, hermitian)\u001b[0m\n\u001b[1;32m   1881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1882\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(A\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m-> 1883\u001b[0m S \u001b[38;5;241m=\u001b[39m \u001b[43msvd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_uv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhermitian\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhermitian\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1885\u001b[0m     tol \u001b[38;5;241m=\u001b[39m S\u001b[38;5;241m.\u001b[39mmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mmax\u001b[39m(A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]) \u001b[38;5;241m*\u001b[39m finfo(S\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39meps\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36msvd\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/Github/DVC_TEST/env/lib/python3.8/site-packages/numpy/linalg/linalg.py:1654\u001b[0m, in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, hermitian)\u001b[0m\n\u001b[1;32m   1651\u001b[0m     gufunc \u001b[38;5;241m=\u001b[39m _umath_linalg\u001b[38;5;241m.\u001b[39msvd_n\n\u001b[1;32m   1653\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->d\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 1654\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[43mgufunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1655\u001b[0m s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mastype(_realType(result_t), copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m s\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "print(f\"Building dataset for {MODEL_TYPE}.\")\n",
    "model_prep = prepare_data_walk_forward(df, MODEL_TYPE)\n",
    "with open(f'../data/{MODEL_TYPE}_dataset.pkl', 'wb') as handle:\n",
    "    pickle.dump(model_prep, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
