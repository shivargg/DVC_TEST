{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation Notebook\n",
    "1. Load data from Snowflake.\n",
    "2. Performs relevant checks and operations (removing columns with one value, VIF checks, scaling) to get a dataset ready for modelling for the following agent feature - API.\n",
    "3. Stores all relevent data as a pkl object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read params from yaml file.\n",
    "params = yaml.safe_load(open('params.yaml'))['prepare']\n",
    "\n",
    "MODEL_TYPE = params['model_type'] # 'API', 'APP_COUNT', 'FYC' or 'PERSISTENCY'\n",
    "TEST_SET_LENGTH = int(params['test_set_length']) # In years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, MinMaxScaler, RobustScaler\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels .tools.tools import add_constant\n",
    "from pickle import dumps\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import copy\n",
    "import datetime, time\n",
    "import math\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 800)\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake_acc\n",
    "\n",
    "with open('uat_creds.json', 'r') as fp:\n",
    "    params = json.load(fp)\n",
    "\n",
    "acc = snowflake_acc.create_snowflake_accessor(params, globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_version_date_time_str = today.strftime(\"%m_%d_%y__%H_%M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_cols_with_one_unique_value(data):\n",
    "    cols_to_drop = []\n",
    "    all_cols = list(data.columns)\n",
    "    for col in all_cols:\n",
    "        if np.shape(data[col].unique())[0] == 1:\n",
    "            cols_to_drop.append(col)\n",
    "    data.drop(columns = cols_to_drop, inplace = True)\n",
    "    print(\"Columns dropped: \", cols_to_drop)\n",
    "    print(f\"Dropped {len(cols_to_drop)} columns from the dataframe.\")\n",
    "    return cols_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_list(target_value_to_predict):\n",
    "    return [f'{target_value_to_predict}_Q_MINUS_{i}' for i in range(1, 5)]\n",
    "\n",
    "\n",
    "def generate_train_test(data_orig, predictor_col):\n",
    "    data = copy.deepcopy(data_orig)\n",
    "    latest_quarter = pd.to_datetime(data['YEAR_QTR_DATE'].max())\n",
    "    # Remove the most current quarter data.\n",
    "    data = data[data['YEAR_QTR_DATE'] != latest_quarter]\n",
    "    # Construct the time point where we start splitting our data into train and test.\n",
    "    split_point = latest_quarter - pd.DateOffset(years= TEST_SET_LENGTH)\n",
    "    # Build X-Train and y-train.\n",
    "    train = data[data['YEAR_QTR_DATE'] < split_point]\n",
    "#     y_train = X_train[predictor_col]\n",
    "#     X_train.drop(columns = [predictor_col], inplace = True)\n",
    "    # Build X-Test and y-test.\n",
    "    test = data[(data['YEAR_QTR_DATE'] >= split_point)]\n",
    "#     y_test = X_test[predictor_col]\n",
    "#     X_test.drop(columns = [predictor_col], inplace = True)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def split_df_train(data, predictor_col):\n",
    "    qtr_to_split = pd.to_datetime(data['YEAR_QTR_DATE'].max()) - pd.DateOffset(months=3)\n",
    "    # Build X-Train and y-train.\n",
    "    X_train = data[data['YEAR_QTR_DATE'] < qtr_to_split]\n",
    "    y_train = X_train[predictor_col]\n",
    "    X_train.drop(columns = [predictor_col], inplace = True)\n",
    "    # Build X-Test and y-test.\n",
    "    X_test = data[data['YEAR_QTR_DATE'] >= qtr_to_split]\n",
    "    y_test = X_test[predictor_col]\n",
    "    X_test.drop(columns = [predictor_col], inplace = True)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def min_max_scale_X(data_train, data_test, cols_to_scale, test_set_passed):\n",
    "    scaler_train = MinMaxScaler().fit(data_train[cols_to_scale].values)\n",
    "    data_train[cols_to_scale] = pd.DataFrame(scaler_train.transform(data_train[cols_to_scale].values), columns=cols_to_scale, index=data_train.index).fillna(0)\n",
    "    if test_set_passed:\n",
    "        data_test[cols_to_scale] = pd.DataFrame(scaler_train.transform(data_test[cols_to_scale].values), columns=cols_to_scale, index=data_test.index).fillna(0)\n",
    "    return scaler_train\n",
    "\n",
    "\n",
    "def min_max_scale_y(data_train, data_test, test_set_passed):\n",
    "    scaler_train = MinMaxScaler().fit(data_train.values.reshape(-1, 1))\n",
    "    data_train = pd.DataFrame(scaler_train.transform(data_train.values.reshape(-1, 1)), index=data_train.index).fillna(0)\n",
    "    if test_set_passed:\n",
    "        data_test = pd.DataFrame(scaler_train.transform(data_test.values.reshape(-1, 1)), index=data_test.index).fillna(0)\n",
    "    return scaler_train\n",
    "    \n",
    "\n",
    "def scale_data(X_train, X_test, y_train, y_test, numeric_cols):\n",
    "    test_set_passed = X_test is not None and y_test is not None\n",
    "    if test_set_passed:\n",
    "        X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = X_train.copy(deep = True), X_test.copy(deep = True), y_train.copy(deep = True), y_test.copy(deep = True)\n",
    "        scaler_X_train = min_max_scale_X(X_train_scaled, X_test_scaled, numeric_cols, test_set_passed)\n",
    "#         scaler_y_train = min_max_scale_y(y_train_scaled, y_test_scaled, test_set_passed)\n",
    "        return scaler_X_train, None, X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled\n",
    "    else:\n",
    "        X_scaled, y_scaled = X_train.copy(deep = True), y_train.copy(deep = True)\n",
    "        scaler_X_train = min_max_scale_X(X_scaled, None, numeric_cols, test_set_passed)\n",
    "#         scaler_y_train = min_max_scale_y(y_scaled, None, test_set_passed)\n",
    "        return scaler_X_train, None, X_scaled, y_scaled\n",
    "\n",
    "\n",
    "def calc_max_vif(data):\n",
    "    vif = pd.DataFrame()\n",
    "    all_cols = data.columns\n",
    "    vif_values = [variance_inflation_factor(data.values, i) for i in range(len(all_cols))]\n",
    "    vif = pd.DataFrame({'column': all_cols, 'vif': vif_values})\n",
    "    max_vif_col = vif.loc[vif['vif'].idxmax()]\n",
    "    return max_vif_col\n",
    "\n",
    "\n",
    "def find_cols_to_remove_vif(X_train, numeric_cols):\n",
    "    vif_cols_dropped = []\n",
    "    cols_not_to_drop = ['Quarter__1', 'Quarter__2', 'Quarter__3', 'Quarter__4', 'AGT_F_CLI_COUNT', 'AGT_M_CLI_COUNT', 'NUM_ACTIVE_POLICIES']\n",
    "    X_train_VIF = X_train.drop(columns = cols_not_to_drop)\n",
    "    numeric_cols_clean = copy.deepcopy(numeric_cols)\n",
    "    for el in cols_not_to_drop:\n",
    "        numeric_cols_clean.remove(el)\n",
    "    while True:\n",
    "        highest_vif = calc_max_vif(X_train_VIF[numeric_cols_clean])\n",
    "        if highest_vif['vif'] > 5.0:\n",
    "            print(highest_vif)\n",
    "            col_to_remove = highest_vif['column']\n",
    "            X_train_VIF.drop(columns = [col_to_remove], inplace = True)\n",
    "            numeric_cols_clean.remove(col_to_remove)\n",
    "            vif_cols_dropped.append(col_to_remove)\n",
    "            print(\"\\n\")\n",
    "        else:\n",
    "            break\n",
    "    return vif_cols_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "\n",
    "def calculate_vif(X, thresh=100, verbose=False):\n",
    "    cols_to_drop = []\n",
    "    cols = X.columns\n",
    "    variables = np.arange(X.shape[1])\n",
    "    dropped = True\n",
    "    while dropped:\n",
    "        dropped = False\n",
    "        c = X[cols[variables]].values\n",
    "        vif = [variance_inflation_factor(c, ix) for ix in np.arange(c.shape[1])]\n",
    "        maxloc = vif.index(max(vif))\n",
    "        if max(vif) > thresh:\n",
    "            cols_to_drop.append(X[cols[variables]].columns[maxloc])\n",
    "            if verbose:\n",
    "                print('dropping \\'' + X[cols[variables]].columns[maxloc] + '\\' at index: ' + str(maxloc))\n",
    "            variables = np.delete(variables, maxloc)\n",
    "            dropped = True\n",
    "    if verbose:\n",
    "        print('Remaining variables:')\n",
    "        print(X.columns[variables])\n",
    "    return cols_to_drop #X[cols[variables]]\n",
    "\n",
    "\n",
    "def variance_threshold_selector(data, threshold=0.5):\n",
    "    # https://stackoverflow.com/a/39813304/1956309\n",
    "    selector = VarianceThreshold(threshold)\n",
    "    selector.fit(data)\n",
    "    return data[data.columns[selector.get_support(indices=True)]]\n",
    "\n",
    "\n",
    "def remove_correlated_features(df: pd.DataFrame, inplace=False):\n",
    "    corr_matrix = df.corr().abs()\n",
    "\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "    # Find features with correlation greater than 0.95\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "    if inplace:\n",
    "        df.drop(to_drop, axis=1, inplace=True)\n",
    "        return df, to_drop\n",
    "\n",
    "    df1 = df.drop(to_drop, axis=1, inplace=False)\n",
    "    return df1, to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quarter_to_predict(df, quarters_to_lookback):\n",
    "    all_quarters = df['YEAR_QTR_DATE'].unique()\n",
    "    all_quarters.sort()\n",
    "    quarter_to_predict = all_quarters[-1 * quarters_to_lookback]\n",
    "    return quarter_to_predict\n",
    "\n",
    "\n",
    "def extract_year_qtr(df, quarter_to_predict):\n",
    "    return df.query(f\"YEAR_QTR_DATE < '{pd.to_datetime(quarter_to_predict)}'\")\n",
    "\n",
    "\n",
    "def build_dataset_final(X, y, agt_ids, quarters_to_lookback):\n",
    "    train = copy.deepcopy(X)\n",
    "    train['y'] = y\n",
    "    train['AGT_ID'] = agt_ids\n",
    "    train.reset_index(inplace = True, drop = True)\n",
    "    quarter_to_predict = get_quarter_to_predict(X, 1)\n",
    "    print(\"QUARTER TO PREDICT -> \", quarter_to_predict)\n",
    "    train_filtered = extract_year_qtr(train, quarter_to_predict)\n",
    "    year_qtr_max = train_filtered['YEAR_QTR_DATE'].max()\n",
    "    X_train = train_filtered.drop(columns = ['YEAR_QTR_DATE', 'y', 'AGT_ID'])\n",
    "    y_train = train_filtered['y']\n",
    "    final = train.query(f\"YEAR_QTR_DATE >= '{quarter_to_predict}'\")\n",
    "    X_final = final.drop(columns = ['YEAR_QTR_DATE', 'y'])\n",
    "    print(X_final.shape)\n",
    "    return X_train, y_train, X_final, quarter_to_predict\n",
    "\n",
    "\n",
    "def get_current_year_quarter():\n",
    "    current_time = datetime.datetime.now()\n",
    "    month = current_time.month\n",
    "    return f'{current_time.year}_{math.floor(month/3) + 1}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track Columns Dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols_dropped = []\n",
    "all_cat_cols = []\n",
    "all_numeric_cols = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc.switch_schema('05_MODEL_INPUT_TOP_AGENT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing\n",
      "Processing table with  106 columns\n",
      "Processed  6860  rows\n"
     ]
    }
   ],
   "source": [
    "df_lagged = acc.retrieve_query_res(f'SELECT * FROM DS_GLOC_DEV_DB.\"05_MODEL_INPUT_TOP_AGENT\".\"05_AGENT_MASTER_TABLE_CLEANED\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing\n",
      "Processing table with  106 columns\n",
      "Processed  7486  rows\n"
     ]
    }
   ],
   "source": [
    "df_score = acc.retrieve_query_res(f'SELECT * FROM DS_GLOC_DEV_DB.\"05_MODEL_INPUT_TOP_AGENT\".\"05_AGENT_MASTER_TABLE_CLEANED_SCORING\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.fromtimestamp(time.time())\n",
    "curr_quarter = (now.month-1)//3+1\n",
    "curr_year = now.year\n",
    "\n",
    "data_quarter = int(df_lagged['YEAR_QUARTER'].max().split('_')[-1])\n",
    "data_quarter_date = df_lagged['YEAR_QUARTER'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019_1'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lagged['YEAR_QUARTER'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022_3'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_quarter_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(581, 106)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lagged[df_lagged['YEAR_QUARTER'] == data_quarter_date].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Removing last year quarter data from df_lagged.\")\n",
    "# df_lagged = df_lagged[df_lagged['YEAR_QUARTER'] != data_quarter_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding the last year quarter from df_scoring.\n"
     ]
    }
   ],
   "source": [
    "# Add the last quarter from the scoring set.\n",
    "print(\"Adding the last year quarter from df_scoring.\")\n",
    "df_score = df_score[df_score['YEAR_QUARTER'] == data_quarter_date]\n",
    "df_score['YEAR_QUARTER'] = f'{curr_year}_{curr_quarter}'\n",
    "df_score['QUARTER'] = curr_quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(598, 106)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022_3'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lagged['YEAR_QUARTER'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019_1'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lagged['YEAR_QUARTER'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022_4'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_score['YEAR_QUARTER'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022_4'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_score['YEAR_QUARTER'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(598, 106)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6860, 106)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lagged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_lagged, df_score], ignore_index = True, join = 'inner', verify_integrity = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7458, 106)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7458 entries, 0 to 7457\n",
      "Data columns (total 106 columns):\n",
      " #    Column                            Non-Null Count  Dtype  \n",
      "---   ------                            --------------  -----  \n",
      " 0    AGT_ID                            7458 non-null   object \n",
      " 1    AGT_GIV_NM                        7155 non-null   object \n",
      " 2    AGT_SUR_NM                        7458 non-null   object \n",
      " 3    AGT_FULL_NAME                     7155 non-null   object \n",
      " 4    AGT_FULL_NAME_NON_NULL            7458 non-null   object \n",
      " 5    AGT_BRANCH                        7458 non-null   object \n",
      " 6    AGT_AGE                           7458 non-null   object \n",
      " 7    AGT_INCM_EARN_AMT                 7458 non-null   int64  \n",
      " 8    AGT_EMPL_YR_QTY                   7458 non-null   object \n",
      " 9    AGT_SEX_F                         7458 non-null   int64  \n",
      " 10   AGT_SEX_M                         7458 non-null   int64  \n",
      " 11   AGT_SEX_C                         7458 non-null   int64  \n",
      " 12   CLI_OCCP_CLAS_1                   7458 non-null   int64  \n",
      " 13   CLI_OCCP_CLAS_2                   7458 non-null   int64  \n",
      " 14   CLI_OCCP_CLAS_MISSING             7458 non-null   int64  \n",
      " 15   AGT_MARIT_STAT_S                  7458 non-null   int64  \n",
      " 16   AGT_MARIT_STAT_C                  7458 non-null   int64  \n",
      " 17   AGT_MARIT_STAT_W                  7458 non-null   int64  \n",
      " 18   AGT_MARIT_STAT_M                  7458 non-null   int64  \n",
      " 19   AGT_MARIT_STAT_D                  7458 non-null   int64  \n",
      " 20   AGT_MARIT_STAT_P                  7458 non-null   int64  \n",
      " 21   AGT_MARIT_STAT_MISSING            7458 non-null   int64  \n",
      " 22   AGT_REGION_SIPARIA                7458 non-null   int64  \n",
      " 23   AGT_REGION_SANGRE_GRANDE          7458 non-null   int64  \n",
      " 24   AGT_REGION_TOBAGO                 7458 non-null   int64  \n",
      " 25   AGT_REGION_PENALDEBE              7458 non-null   int64  \n",
      " 26   AGT_REGION_SAN_JUAN_LAVENTILLE    7458 non-null   int64  \n",
      " 27   AGT_REGION_TUNAPUNAPIARCO         7458 non-null   int64  \n",
      " 28   AGT_REGION_CHAGUANAS              7458 non-null   int64  \n",
      " 29   AGT_REGION_COUVATABAQUITETALPARO  7458 non-null   int64  \n",
      " 30   AGT_REGION_PRINCES_TOWN           7458 non-null   int64  \n",
      " 31   AGT_REGION_POINT_FORTIN           7458 non-null   int64  \n",
      " 32   AGT_REGION_PORT_OF_SPAIN          7458 non-null   int64  \n",
      " 33   AGT_REGION_MISSING                7458 non-null   int64  \n",
      " 34   AGT_REGION_DIEGO_MARTIN           7458 non-null   int64  \n",
      " 35   AGT_REGION_ARIMA                  7458 non-null   int64  \n",
      " 36   AGT_REGION_SAN_FERNANDO           7458 non-null   int64  \n",
      " 37   AGT_REGION_MAYARORIO_CLARO        7458 non-null   int64  \n",
      " 38   AGT_REGION_OTHER                  7458 non-null   int64  \n",
      " 39   AGT_PART_TIME_FLAG                7458 non-null   int64  \n",
      " 40   AGT_AGE_TEMPORAL                  7458 non-null   object \n",
      " 41   YEAR                              7458 non-null   int64  \n",
      " 42   QUARTER                           7458 non-null   int64  \n",
      " 43   YEAR_QUARTER                      7458 non-null   object \n",
      " 44   APP_COUNT                         7458 non-null   object \n",
      " 45   FYC                               7458 non-null   object \n",
      " 46   PERSISTENCY                       7458 non-null   object \n",
      " 47   API                               7458 non-null   object \n",
      " 48   APP_COUNT_Q_MINUS_1               7458 non-null   object \n",
      " 49   APP_COUNT_Q_MINUS_2               7458 non-null   object \n",
      " 50   APP_COUNT_Q_MINUS_3               7458 non-null   object \n",
      " 51   APP_COUNT_Q_MINUS_4               7458 non-null   object \n",
      " 52   FYC_Q_MINUS_1                     7458 non-null   object \n",
      " 53   FYC_Q_MINUS_2                     7458 non-null   object \n",
      " 54   FYC_Q_MINUS_3                     7458 non-null   object \n",
      " 55   FYC_Q_MINUS_4                     7458 non-null   object \n",
      " 56   PERSISTENCY_Q_MINUS_1             7458 non-null   object \n",
      " 57   PERSISTENCY_Q_MINUS_2             7458 non-null   object \n",
      " 58   PERSISTENCY_Q_MINUS_3             7458 non-null   object \n",
      " 59   PERSISTENCY_Q_MINUS_4             7458 non-null   object \n",
      " 60   API_Q_MINUS_1                     7458 non-null   object \n",
      " 61   API_Q_MINUS_2                     7458 non-null   object \n",
      " 62   API_Q_MINUS_3                     7458 non-null   object \n",
      " 63   API_Q_MINUS_4                     7458 non-null   object \n",
      " 64   AGENT_TENURE                      7458 non-null   int64  \n",
      " 65   COUNT_3                           7456 non-null   float64\n",
      " 66   COUNT_7                           7456 non-null   float64\n",
      " 67   COUNT_8                           7456 non-null   float64\n",
      " 68   COUNT_9                           7456 non-null   float64\n",
      " 69   COUNT_C                           7456 non-null   float64\n",
      " 70   COUNT_D                           7456 non-null   float64\n",
      " 71   COUNT_F                           7456 non-null   float64\n",
      " 72   COUNT_M                           7456 non-null   float64\n",
      " 73   COUNT_N                           7456 non-null   float64\n",
      " 74   COUNT_V                           7456 non-null   float64\n",
      " 75   AGT_CLI_MEDIAN_YRS_EMPLOYED       7458 non-null   object \n",
      " 76   AGT_CLI_AVG_YRS_EMPLOYED          7458 non-null   float64\n",
      " 77   AGT_CLI_MEDIAN_INCOME             7458 non-null   object \n",
      " 78   AGT_CLI_AVG_INCOME                7458 non-null   object \n",
      " 79   AGT_F_CLI_COUNT                   7458 non-null   int64  \n",
      " 80   AGT_M_CLI_COUNT                   7458 non-null   int64  \n",
      " 81   AGT_O_CLI_COUNT                   7458 non-null   int64  \n",
      " 82   NUM_SURRENDERED_POLICIES          7458 non-null   int64  \n",
      " 83   NUM_REJECTED_POLICIES             7458 non-null   int64  \n",
      " 84   NUM_REDATED_POLICIES              7458 non-null   int64  \n",
      " 85   NUM_LAPSED_POLICIES               7458 non-null   int64  \n",
      " 86   NUM_ACTIVE_POLICIES               7458 non-null   int64  \n",
      " 87   NUM_PENDING_POLICIES              7458 non-null   int64  \n",
      " 88   CVG_FACE_AMT                      7458 non-null   object \n",
      " 89   CVG_ORIG_FACE_AMT                 7458 non-null   object \n",
      " 90   CVG_PREV_FACE_AMT                 7458 non-null   object \n",
      " 91   CVG_UWG_AMT                       7458 non-null   object \n",
      " 92   CVG_UNIT_VALU_AMT                 7458 non-null   int64  \n",
      " 93   CVG_SUM_INS_AMT                   7458 non-null   object \n",
      " 94   CVG_MPREM_AMT                     7458 non-null   object \n",
      " 95   CVG_PFEE_AMT                      7458 non-null   object \n",
      " 96   CVG_BASIC_PREM_AMT                7458 non-null   object \n",
      " 97   CVG_MDRT_AMT                      7458 non-null   int64  \n",
      " 98   CVG_FYR_COMM_AMT                  7458 non-null   object \n",
      " 99   IN_ALLOC_AMT_PCT                  7458 non-null   object \n",
      " 100  CVG_PMT_LTD_AMT                   7458 non-null   object \n",
      " 101  MNPMT_TRG_LTD_AMT                 7458 non-null   int64  \n",
      " 102  SURR_LOAD_LTD_AMT                 7458 non-null   object \n",
      " 103  CVG_SURR_LTD_AMT                  7458 non-null   object \n",
      " 104  CVG_GDLN_APREM_AMT                7458 non-null   object \n",
      " 105  DATE_OF_BIRTH                     3427 non-null   object \n",
      "dtypes: float64(11), int64(47), object(48)\n",
      "memory usage: 6.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info(verbose = True, show_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['AGT_SEX_F', 'AGT_SEX_M',\n",
    "            'CLI_OCCP_CLAS_1', 'CLI_OCCP_CLAS_2',\n",
    "            'AGT_MARIT_STAT_S', 'AGT_MARIT_STAT_C', 'AGT_MARIT_STAT_W', 'AGT_MARIT_STAT_M', 'AGT_MARIT_STAT_D', 'AGT_MARIT_STAT_P',\n",
    "            'AGT_REGION_TUNAPUNAPIARCO', 'AGT_REGION_TOBAGO', 'AGT_REGION_SIPARIA', 'AGT_REGION_SAN_JUAN_LAVENTILLE', 'AGT_REGION_SAN_FERNANDO',\n",
    "            'AGT_REGION_SANGRE_GRANDE', 'AGT_REGION_PRINCES_TOWN', 'AGT_REGION_PORT_OF_SPAIN', 'AGT_REGION_POINT_FORTIN', 'AGT_REGION_PENALDEBE',\n",
    "            'AGT_REGION_OTHER', 'AGT_REGION_MISSING', 'AGT_REGION_MAYARORIO_CLARO', 'AGT_REGION_DIEGO_MARTIN', 'AGT_REGION_COUVATABAQUITETALPARO',\n",
    "            'AGT_REGION_CHAGUANAS', 'AGT_REGION_ARIMA',\n",
    "            'AGT_PART_TIME_FLAG'\n",
    "]\n",
    "\n",
    "cols_to_drop = ['AGT_SEX_C', \n",
    "                'CLI_OCCP_CLAS_MISSING', \n",
    "                'AGT_MARIT_STAT_MISSING', \n",
    "                'DATE_OF_BIRTH',\n",
    "                'AGT_FULL_NAME', 'AGT_FULL_NAME_NON_NULL', 'AGT_GIV_NM', 'AGT_SUR_NM', \n",
    "                'YEAR', 'MONTH', 'YEAR_QUARTER', \n",
    "                'AGT_AGE', 'AGT_BRANCH'\n",
    "]\n",
    "\n",
    "cols_to_drop_fluid = [\n",
    "                'CNTRCT_TRMN_DT_TXT', 'AGT_STAT_CD', 'START_MONTH', 'START_YEAR',\n",
    "                'FEEDBACK', 'MAX(CFCVG.POL_ID)', 'MAX(CFCVG.CVG_NUM)',\n",
    "#                 'AGT_M_CLI_COUNT', 'AGT_F_CLI_COUNT', 'AGT_O_CLI_COUNT', \n",
    "                'AGT_TOTAL_CLI_INCM_EARNED', 'AGT_CLI_AVG_EMPL_AGE', 'AGT_CLI_OCCP_CLAS_CD_COUNT', 'AGT_CLI_MARIT_STAT_CD_COUNT',\n",
    "                'CVG_FACE_AMT', 'CVG_ORIG_FACE_AMT', 'CVG_PREV_FACE_AMT', 'CVG_UWG_AMT', 'CVG_UNIT_VALU_AMT', 'CVG_SUM_INS_AMT', 'CVG_AD_FACE_AMT', 'CVG_MPREM_AMT', 'CVG_PFEE_AMT',\n",
    "                'CVG_BASIC_PREM_AMT', 'CVG_AD_PREM_AMT', 'CVG_WP_PREM_AMT', 'REDC_EP_PREM_AMT', 'OWN_OCCP_PREM_AMT', 'CVG_LTA_PREM_AMT', 'CVG_LTB_PREM_AMT', 'PDISAB_PREM_AMT', 'CVG_COLA_PREM_AMT',\n",
    "                'CVG_FE_UPREM_AMT', 'CVG_FE_PREM_AMT', 'CVG_ME_PREM_AMT', 'CVG_SALE_TAX_AMT', 'PREV_WP_UPREM_AMT', 'CVG_PREV_UPREM_AMT', 'CVG_NXT_UPREM_AMT', 'CVG_MDRT_AMT', 'CVG_FYR_COMM_AMT',\n",
    "                'CVG_CLM_YTD_AMT', 'CVG_CLM_LTD_AMT', 'CVG_CLM_CHQ_AMT', 'CVG_WP_YTD_AMT', 'CVG_WP_LTD_AMT', 'CVG_NET_REISS_AMT', \n",
    "                'IN_ALLOC_AMT_PCT', 'OUT_ALLOC_AMT_PCT', 'CVG_MAX_COMIT_AMT',\n",
    "                'CVG_COMM_TRG_AMT', 'PMT_LOAD_TRG_AMT', 'PMT_LOAD_LTD_AMT', 'CVG_PMT_LTD_AMT', 'MNPMT_TRG_LTD_AMT', 'CVG_SURR_TRG_AMT', 'SURR_LOAD_LTD_AMT', 'CVG_SURR_LTD_AMT',\n",
    "                'CVG_GDLN_APREM_AMT', 'CVG_GDLN_SPREM_AMT', 'CVG_LOAN_CLR_1_AMT', 'CVG_LOAN_CLR_2_AMT', 'CVG_APL_CLR_AMT', 'GIR_OPT_REMN_AMT', 'CVG_FE2_UPREM_AMT', '2018_TOTAL_BONUS_RATE',\n",
    "                '2018_TOTAL_ADD_BONUS_RATE', '2019_TOTAL_BONUS_RATE', '2019_TOTAL_ADD_BONUS_RATE', '2020_TOTAL_BONUS_RATE', '2020_TOTAL_ADD_BONUS_RATE', '2021_TOTAL_BONUS_RATE', '2021_TOTAL_ADD_BONUS_RATE',\n",
    "                '2018_FYR_COMM_CMO_AMT', '2018_FYR_COMM_YTD_AMT', '2018_RENW_COMM_CMO_AMT', '2018_RENW_COMM_YTD_AMT', '2018_OVRID_COMM_CMO_AMT', '2018_OVRID_COMM_YTD_AMT', '2018_AGT_PAYO_CMO_AMT',\n",
    "                '2018_AGT_PAYO_YTD_AMT', '2018_FYR_LCOMM_CMO_AMT', '2018_FYR_LCOMM_YTD_AMT', '2018_RENW_LCOMM_CMO_AMT', '2018_RENW_LCOMM_YTD_AMT', '2018_QLTY_BON_CMO_AMT', '2018_QLTY_BON_YTD_AMT',\n",
    "                '2018_MKT_BON_CMO_AMT', '2018_MKT_BON_YTD_AMT', '2018_NHS_ALLOW_CMO_AMT', '2018_NHS_ALLOW_YTD_AMT', '2018_AGT_APP_PMO_QTY', '2018_AGT_APP_YTM_QTY', '2018_AGT_PWRIT_PMO_AMT',\n",
    "                '2018_AGT_PWRIT_YTM_AMT', '2018_LIFE_PWRIT_PMO_AMT', '2018_LIFE_PWRIT_YTM_AMT', '2018_FYR_CPREM_CMO_AMT', '2018_FYR_CPREM_YTD_AMT', '2018_RENW_CPREM_CMO_AMT',\n",
    "                '2018_RENW_CPREM_YTD_AMT', '2018_TOTAL_FYR_COMM', '2018_TOTAL_RENW_COMM', '2019_FYR_COMM_CMO_AMT', '2019_FYR_COMM_YTD_AMT', '2019_RENW_COMM_CMO_AMT', '2019_RENW_COMM_YTD_AMT',\n",
    "                '2019_OVRID_COMM_CMO_AMT', '2019_OVRID_COMM_YTD_AMT', '2019_AGT_PAYO_CMO_AMT', '2019_AGT_PAYO_YTD_AMT', '2019_FYR_LCOMM_CMO_AMT', '2019_FYR_LCOMM_YTD_AMT', '2019_RENW_LCOMM_CMO_AMT',\n",
    "                '2019_RENW_LCOMM_YTD_AMT', '2019_QLTY_BON_CMO_AMT', '2019_QLTY_BON_YTD_AMT', '2019_MKT_BON_CMO_AMT', '2019_MKT_BON_YTD_AMT', '2019_NHS_ALLOW_CMO_AMT', '2019_NHS_ALLOW_YTD_AMT',\n",
    "                '2019_AGT_APP_PMO_QTY', '2019_AGT_APP_YTM_QTY', '2019_AGT_PWRIT_PMO_AMT', '2019_AGT_PWRIT_YTM_AMT', '2019_LIFE_PWRIT_PMO_AMT', '2019_LIFE_PWRIT_YTM_AMT', '2019_FYR_CPREM_CMO_AMT',\n",
    "                '2019_FYR_CPREM_YTD_AMT', '2019_RENW_CPREM_CMO_AMT', '2019_RENW_CPREM_YTD_AMT', '2019_TOTAL_FYR_COMM', '2019_TOTAL_RENW_COMM', '2020_FYR_COMM_CMO_AMT', '2020_FYR_COMM_YTD_AMT', '2020_RENW_COMM_CMO_AMT',\n",
    "                '2020_RENW_COMM_YTD_AMT', '2020_OVRID_COMM_CMO_AMT', '2020_OVRID_COMM_YTD_AMT', '2020_AGT_PAYO_CMO_AMT', '2020_AGT_PAYO_YTD_AMT', '2020_FYR_LCOMM_CMO_AMT', '2020_FYR_LCOMM_YTD_AMT',\n",
    "                '2020_RENW_LCOMM_CMO_AMT', '2020_RENW_LCOMM_YTD_AMT', '2020_QLTY_BON_CMO_AMT', '2020_QLTY_BON_YTD_AMT', '2020_MKT_BON_CMO_AMT', '2020_MKT_BON_YTD_AMT', '2020_NHS_ALLOW_CMO_AMT',\n",
    "                '2020_NHS_ALLOW_YTD_AMT', '2020_AGT_APP_PMO_QTY', '2020_AGT_APP_YTM_QTY', '2020_AGT_PWRIT_PMO_AMT', '2020_AGT_PWRIT_YTM_AMT', '2020_LIFE_PWRIT_PMO_AMT', '2020_LIFE_PWRIT_YTM_AMT',\n",
    "                '2020_FYR_CPREM_CMO_AMT', '2020_FYR_CPREM_YTD_AMT', '2020_RENW_CPREM_CMO_AMT', '2020_RENW_CPREM_YTD_AMT', '2020_TOTAL_FYR_COMM', '2020_TOTAL_RENW_COMM', '2021_FYR_COMM_CMO_AMT',\n",
    "                '2021_FYR_COMM_YTD_AMT', '2021_RENW_COMM_CMO_AMT', '2021_RENW_COMM_YTD_AMT', '2021_OVRID_COMM_CMO_AMT', '2021_OVRID_COMM_YTD_AMT', '2021_AGT_PAYO_CMO_AMT', '2021_AGT_PAYO_YTD_AMT',\n",
    "                '2021_FYR_LCOMM_CMO_AMT', '2021_FYR_LCOMM_YTD_AMT', '2021_RENW_LCOMM_CMO_AMT', '2021_RENW_LCOMM_YTD_AMT', '2021_QLTY_BON_CMO_AMT', '2021_QLTY_BON_YTD_AMT', '2021_MKT_BON_CMO_AMT', '2021_MKT_BON_YTD_AMT',\n",
    "                '2021_NHS_ALLOW_CMO_AMT', '2021_NHS_ALLOW_YTD_AMT', '2021_AGT_APP_PMO_QTY', '2021_AGT_APP_YTM_QTY', '2021_AGT_PWRIT_PMO_AMT', '2021_AGT_PWRIT_YTM_AMT', '2021_LIFE_PWRIT_PMO_AMT', '2021_LIFE_PWRIT_YTM_AMT',\n",
    "                '2021_FYR_CPREM_CMO_AMT', '2021_FYR_CPREM_YTD_AMT', '2021_RENW_CPREM_CMO_AMT', '2021_RENW_CPREM_YTD_AMT', '2021_TOTAL_FYR_COMM', '2021_TOTAL_RENW_COMM', '3','7', '8',\n",
    "                '9', 'C', 'D', 'F', 'M', 'N', 'V'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols_to_drop_clean = [el for el in cols_to_drop_fluid if 'CVG' not in el]\n",
    "cols_to_drop_clean = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations to Prepare Data:\n",
    "1. Cast columns to numeric form. ()\n",
    "2. Extract quarter. ()\n",
    "3. Dropping cols (multiple times). ()\n",
    "4. Drop rows with missing data. ()\n",
    "5. Build list of numeric columns. ()\n",
    "6. Drop predictor variables. ()\n",
    "7. Split dataset into train and prediction sets. ()\n",
    "8. Split train dataset into train and test segments. ()\n",
    "9. Apply scaling to split data. ()\n",
    "10. VIF removal. ()\n",
    "11. Drop agent feature from datasets. ()\n",
    "12. Save as pickle. ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For validation/testing - remove the current year_quarter value.\n",
    "# For production, include the current year_quarter_value.\n",
    "\n",
    "def prepare_data_walk_forward(df_original, selected_predictor):\n",
    "    # Copy original dataframe.\n",
    "    df = copy.deepcopy(df_original)\n",
    "    \n",
    "    # Attempt to cast all columns to numeric form.\n",
    "    df[df.columns] = df[df.columns].apply(pd.to_numeric, errors='ignore')\n",
    "    \n",
    "    # Applying transformations to extract the quarter.\n",
    "    df['YEAR'] = df['YEAR_QUARTER'].str.split('_').str.get(0)\n",
    "    df['MONTH'] = df['YEAR_QUARTER'].str.split('_').str.get(1).map({'1': '1', '2': '4', '3': '7', '4': '10'})\n",
    "    df['YEAR_QTR_DATE'] = pd.to_datetime(df['YEAR'] + '/' + df['MONTH'] + '/01', format = '%Y/%m/%d')\n",
    "    df['QUARTER'] = df['YEAR_QTR_DATE'].dt.quarter\n",
    "    quarter_dummies = pd.get_dummies(df['QUARTER'], prefix='Quarter_')\n",
    "    df = pd.merge(\n",
    "        left = df,\n",
    "        right = quarter_dummies,\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    "    df.drop(columns = ['QUARTER'], inplace = True)\n",
    "    \n",
    "    # Drop initial set of columns.\n",
    "    df.drop(columns = cols_to_drop, inplace = True)\n",
    "    all_cols_dropped.append(cols_to_drop)\n",
    "    \n",
    "    # Drop columns with one value.\n",
    "    cols_dropped = remove_cols_with_one_unique_value(df)\n",
    "    all_cols_dropped.append(cols_dropped)\n",
    "    \n",
    "    # Drop additional columns.\n",
    "    additional_cols_to_consider_removing = list(set(list(df.columns)).intersection(set(cols_to_drop_clean)))\n",
    "    df.drop(columns = additional_cols_to_consider_removing, inplace = True)\n",
    "    all_cols_dropped.append(additional_cols_to_consider_removing)\n",
    "    \n",
    "    # Drop any rows with missing data.\n",
    "    df.dropna(axis=0, how='any', inplace = True)\n",
    "    \n",
    "    # Build the list of numeric columns.\n",
    "    numeric_cols = set(df.columns).difference(set(cat_cols))\n",
    "    numeric_cols.remove('AGT_ID')\n",
    "    numeric_cols.remove('YEAR_QTR_DATE')\n",
    "    numeric_cols = list(numeric_cols)\n",
    "    \n",
    "    # Drop predictor variables from dataset except the selected_predictor.\n",
    "    all_predictors = ['API', 'PERSISTENCY', 'APP_COUNT', 'FYC']\n",
    "    for predictor in all_predictors:\n",
    "        numeric_cols.remove(predictor)\n",
    "    all_predictors.remove(MODEL_TYPE)\n",
    "    df.drop(columns = all_predictors, inplace = True)\n",
    "    all_cols_dropped.append(all_predictors)\n",
    "    \n",
    "    # We want to create the initial train and test datasets.\n",
    "    train, test = generate_train_test(df, selected_predictor)\n",
    "    \n",
    "    # Run VIF for training data.\n",
    "    vif_cols_5 = calculate_vif(train.drop(columns = ['AGT_ID', 'YEAR_QTR_DATE', selected_predictor]), 5)\n",
    "    \n",
    "    dataset = {\n",
    "        'train': train,\n",
    "        'test': test,\n",
    "        'vif_cols_5': vif_cols_5,\n",
    "        'numeric_cols': numeric_cols,\n",
    "        'cat_cols': cat_cols,\n",
    "        'dropped_cols': all_cols_dropped\n",
    "    }\n",
    "    \n",
    "    print(\"Ranges for train data: \", train['YEAR_QTR_DATE'].min(), train['YEAR_QTR_DATE'].max())\n",
    "    print(\"Ranges for test data: \", train['YEAR_QTR_DATE'].min(), train['YEAR_QTR_DATE'].max())\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Pickle Files / Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataset for API.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_740/643652696.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['MONTH'] = df['YEAR_QUARTER'].str.split('_').str.get(1).map({'1': '1', '2': '4', '3': '7', '4': '10'})\n",
      "/tmp/ipykernel_740/643652696.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['YEAR_QTR_DATE'] = pd.to_datetime(df['YEAR'] + '/' + df['MONTH'] + '/01', format = '%Y/%m/%d')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns dropped:  ['CLI_OCCP_CLAS_2', 'AGT_MARIT_STAT_C', 'AGT_MARIT_STAT_W', 'AGT_REGION_SANGRE_GRANDE', 'AGT_REGION_PENALDEBE', 'AGT_REGION_SAN_JUAN_LAVENTILLE', 'AGT_REGION_TUNAPUNAPIARCO', 'AGT_REGION_COUVATABAQUITETALPARO', 'AGT_REGION_PRINCES_TOWN', 'AGT_REGION_POINT_FORTIN', 'AGT_REGION_PORT_OF_SPAIN', 'AGT_REGION_DIEGO_MARTIN', 'AGT_REGION_SAN_FERNANDO', 'AGT_REGION_MAYARORIO_CLARO']\n",
      "Dropped 14 columns from the dataframe.\n",
      "Ranges for train data:  2019-01-01 00:00:00 2020-07-01 00:00:00\n",
      "Ranges for test data:  2019-01-01 00:00:00 2020-07-01 00:00:00\n",
      "\n",
      "\n",
      "Building dataset for FYC.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_740/643652696.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['MONTH'] = df['YEAR_QUARTER'].str.split('_').str.get(1).map({'1': '1', '2': '4', '3': '7', '4': '10'})\n",
      "/tmp/ipykernel_740/643652696.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['YEAR_QTR_DATE'] = pd.to_datetime(df['YEAR'] + '/' + df['MONTH'] + '/01', format = '%Y/%m/%d')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns dropped:  ['CLI_OCCP_CLAS_2', 'AGT_MARIT_STAT_C', 'AGT_MARIT_STAT_W', 'AGT_REGION_SANGRE_GRANDE', 'AGT_REGION_PENALDEBE', 'AGT_REGION_SAN_JUAN_LAVENTILLE', 'AGT_REGION_TUNAPUNAPIARCO', 'AGT_REGION_COUVATABAQUITETALPARO', 'AGT_REGION_PRINCES_TOWN', 'AGT_REGION_POINT_FORTIN', 'AGT_REGION_PORT_OF_SPAIN', 'AGT_REGION_DIEGO_MARTIN', 'AGT_REGION_SAN_FERNANDO', 'AGT_REGION_MAYARORIO_CLARO']\n",
      "Dropped 14 columns from the dataframe.\n",
      "Ranges for train data:  2019-01-01 00:00:00 2020-07-01 00:00:00\n",
      "Ranges for test data:  2019-01-01 00:00:00 2020-07-01 00:00:00\n",
      "\n",
      "\n",
      "Building dataset for APP_COUNT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_740/643652696.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['MONTH'] = df['YEAR_QUARTER'].str.split('_').str.get(1).map({'1': '1', '2': '4', '3': '7', '4': '10'})\n",
      "/tmp/ipykernel_740/643652696.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['YEAR_QTR_DATE'] = pd.to_datetime(df['YEAR'] + '/' + df['MONTH'] + '/01', format = '%Y/%m/%d')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns dropped:  ['CLI_OCCP_CLAS_2', 'AGT_MARIT_STAT_C', 'AGT_MARIT_STAT_W', 'AGT_REGION_SANGRE_GRANDE', 'AGT_REGION_PENALDEBE', 'AGT_REGION_SAN_JUAN_LAVENTILLE', 'AGT_REGION_TUNAPUNAPIARCO', 'AGT_REGION_COUVATABAQUITETALPARO', 'AGT_REGION_PRINCES_TOWN', 'AGT_REGION_POINT_FORTIN', 'AGT_REGION_PORT_OF_SPAIN', 'AGT_REGION_DIEGO_MARTIN', 'AGT_REGION_SAN_FERNANDO', 'AGT_REGION_MAYARORIO_CLARO']\n",
      "Dropped 14 columns from the dataframe.\n",
      "Ranges for train data:  2019-01-01 00:00:00 2020-07-01 00:00:00\n",
      "Ranges for test data:  2019-01-01 00:00:00 2020-07-01 00:00:00\n",
      "\n",
      "\n",
      "Building dataset for PERSISTENCY.\n",
      "Columns dropped:  ['CLI_OCCP_CLAS_2', 'AGT_MARIT_STAT_C', 'AGT_MARIT_STAT_W', 'AGT_REGION_SANGRE_GRANDE', 'AGT_REGION_PENALDEBE', 'AGT_REGION_SAN_JUAN_LAVENTILLE', 'AGT_REGION_TUNAPUNAPIARCO', 'AGT_REGION_COUVATABAQUITETALPARO', 'AGT_REGION_PRINCES_TOWN', 'AGT_REGION_POINT_FORTIN', 'AGT_REGION_PORT_OF_SPAIN', 'AGT_REGION_DIEGO_MARTIN', 'AGT_REGION_SAN_FERNANDO', 'AGT_REGION_MAYARORIO_CLARO']\n",
      "Dropped 14 columns from the dataframe.\n",
      "Ranges for train data:  2019-01-01 00:00:00 2020-07-01 00:00:00\n",
      "Ranges for test data:  2019-01-01 00:00:00 2020-07-01 00:00:00\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_740/643652696.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['MONTH'] = df['YEAR_QUARTER'].str.split('_').str.get(1).map({'1': '1', '2': '4', '3': '7', '4': '10'})\n",
      "/tmp/ipykernel_740/643652696.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['YEAR_QTR_DATE'] = pd.to_datetime(df['YEAR'] + '/' + df['MONTH'] + '/01', format = '%Y/%m/%d')\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "print(f\"Building dataset for {MODEL_TYPE}.\")\n",
    "model_prep = prepare_data_walk_forward(df, MODEL_TYPE)\n",
    "with open(f'{MODEL_TYPE}_dataset.pkl', 'wb') as handle:\n",
    "    pickle.dump(model_prep, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
